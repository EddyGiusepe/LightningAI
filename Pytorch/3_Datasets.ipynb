{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"><font color=\"yellow\">Pytorch: Datasets</font></h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"yellow\">Data Scientist.: PhD.Eddy Giusepe Chirinos Isidro</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Neste script vamos a estudar como o Pytorch define nossos datasets.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda install requests,matplotlib --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.9.13\n",
      "IPython version      : 8.13.2\n",
      "\n",
      "numpy     : 1.24.3\n",
      "pandas    : 2.0.1\n",
      "matplotlib: 3.7.1\n",
      "requests  : 2.31.0\n",
      "torch     : 2.0.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark \n",
    "%watermark -v -p numpy,pandas,matplotlib,requests,torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterando Tensores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Lembrar:`\n",
    "\n",
    "<font color=\"orange\">O conjunto de MNIST é um conjunto de Dados muito utilizado na área de `Visão Computacional`. As principais características são:</font>\n",
    "\n",
    "* `Imagens em escala de cinza:` Cada imagem do MNIST tem dimensões de `28x28 pixels` e é representada em escala de cinza. Isso significa que cada pixel pode ter um valor entre `0 e 255`, onde <font color=\"orange\">0 representa o preto</font> e <font color=\"orange\">255 representa o branco</font>.\n",
    "\n",
    "* `Dígitos de 0 a 9:` O conjunto de dados MNIST contém imagens de dígitos manuscritos de `0 a 9`. Cada imagem é rotulada com o dígito correspondente.\n",
    "\n",
    "* `Conjunto de treinamento e teste:` O conjunto de dados MNIST é dividido em dois conjuntos: um conjunto de treinamento e um conjunto de teste. O conjunto de treinamento contém `60.000` imagens, enquanto o conjunto de teste contém `10.000` imagens.\n",
    "\n",
    "* `Equilíbrio de classes:` O MNIST é equilibrado em relação às classes, o que significa que cada dígito (0 a 9) tem uma quantidade semelhante de exemplos no conjunto de dados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eddygiusepe/miniconda3/envs/LightningAI/lib/python3.9/site-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np\n",
    "\n",
    "# Descarregando nosso Dataset\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "X, Y = mnist[\"data\"], mnist[\"target\"]\n",
    "\n",
    "# Normalizando:\n",
    "X_train, X_test, y_train, y_test = X[:60000] / 255., X[60000:] / 255., Y[:60000].astype(int), Y[60000:].astype(int)\n",
    "\n",
    "\n",
    "X_t = torch.from_numpy(X_train.values).float().cuda()\n",
    "Y_t = torch.from_numpy(y_train.values).long().cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_in, H, D_out = 784, 100, 10\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def softmax(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(axis=-1,keepdims=True)\n",
    "\n",
    "def evaluate(x):\n",
    "    model.eval()\n",
    "    y_pred = model(x)\n",
    "    y_probas = softmax(y_pred)\n",
    "    return torch.argmax(y_probas, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100 Loss 1.80345\n",
      "Epoch 20/100 Loss 1.53021\n",
      "Epoch 30/100 Loss 1.25949\n",
      "Epoch 40/100 Loss 1.07717\n",
      "Epoch 50/100 Loss 0.95154\n",
      "Epoch 60/100 Loss 0.85129\n",
      "Epoch 70/100 Loss 0.77467\n",
      "Epoch 80/100 Loss 0.71532\n",
      "Epoch 90/100 Loss 0.66693\n",
      "Epoch 100/100 Loss 0.62649\n",
      "\n",
      "\u001b[93mA accuracy é: \u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9302"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.8)\n",
    "\n",
    "\n",
    "epochs = 100\n",
    "log_each = 10\n",
    "l = []\n",
    "model.train()\n",
    "for e in range(1, epochs+1): \n",
    "    \n",
    "    # forward\n",
    "    y_pred = model(X_t)\n",
    "\n",
    "    # loss\n",
    "    loss = criterion(y_pred, Y_t)\n",
    "    l.append(loss.item())\n",
    "    \n",
    "    # ponemos a cero los gradientes\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backprop (calculamos todos los gradientes automáticamente)\n",
    "    loss.backward()\n",
    "\n",
    "    # update de los pesos\n",
    "    optimizer.step()\n",
    "    \n",
    "    if not e % log_each:\n",
    "        print(f\"Epoch {e}/{epochs} Loss {np.mean(l):.5f}\")\n",
    "        \n",
    "y_pred = evaluate(torch.from_numpy(X_test.values).float().cuda())\n",
    "\n",
    "print(\"\")\n",
    "print(\"\\033[93mA accuracy é: \\033[0m\")\n",
    "accuracy_score(y_test, y_pred.cpu().numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterando por Batches"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Na implementação anterior estamos Otimizando nosso modelo com o Algoritmo de `batch gradient descent`, na qual utilizamos todos nossos Dados em cada passo de Otimização. No entanto, um algoritmo que pode convergir mais rápido (e única opção se nosso dataset é tão grande que não cabe em memória) é o de `mini-batch gradient descent`.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_in, H, D_out = 784, 100, 10\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ").to(\"cuda\")\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.8)\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 100\n",
    "log_each = 1\n",
    "l = []\n",
    "model.train()\n",
    "batches = len(X_t) // batch_size\n",
    "for e in range(1, epochs+1): \n",
    "    \n",
    "    _l = []\n",
    "    # Iteramos por batches\n",
    "    for b in range(batches):\n",
    "        x_b = X_t[b*batch_size:(b+1)*batch_size]\n",
    "        y_b = Y_t[b*batch_size:(b+1)*batch_size]\n",
    "        \n",
    "        # forward\n",
    "        y_pred = model(x_b)\n",
    "\n",
    "        # loss\n",
    "        loss = criterion(y_pred, y_b)\n",
    "        _l.append(loss.item())\n",
    "\n",
    "        # ponemos a cero los gradientes\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backprop (calculamos todos los gradientes automáticamente)\n",
    "        loss.backward()\n",
    "\n",
    "        # update de los pesos\n",
    "        optimizer.step()\n",
    "    \n",
    "    l.append(np.mean(_l))\n",
    "    if not e % log_each:\n",
    "        print(f\"Epoch {e}/{epochs} Loss {np.mean(l):.5f}\")\n",
    "        \n",
    "y_pred = evaluate(torch.from_numpy(X_test.values).float().cuda())\n",
    "\n",
    "print(\"\")\n",
    "print(\"\\033[93mA accuracy é: \\033[0m\")\n",
    "accuracy_score(y_test, y_pred.cpu().numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta implementação é correta e funcional, dependendo de nossos dados pode chegar a ser mais complexa (<font color=\"orange\">`Por exemplo:` se precisarmos carregar muitas imagens à quais queremos aplicar Transformações, juntar batches, etc</font>) Ademais, é comum re-utilizar a lógica para carregar nossos dados não só para treinar a rede, senão para gerar predições. Este fato motiva o uso das Classes especiais que `Pytorch` nos oferece para isso. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A classe Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Começamos estudando a classe `Dataset`. Esta classe herda da classe pai `torch.utils.data.Dataset` e temos que definir, como mínimo, três funções: \n",
    "\n",
    "- `__init__`: o construtor\n",
    "- `__len__`: retorna o número de amostras no dataset\n",
    "- `__getitem__`: retorna uma amostra em concreto do dataset\n",
    "\n",
    "Uma vez definida a classe, ésta pode se usada como se de qualquer iterador se trata-se."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A classe Dataset, herda da classe `torch.utils.data.Dataset`\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    # Construtor\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = torch.from_numpy(X.values).float().cuda()\n",
    "        self.Y = torch.from_numpy(Y.values).long().cuda() # .long() --> Passa para Inteiro Longo (para representar comumente Rótulos ou Categorias)\n",
    "    # Retornamos o número de dados no dataset\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    # Retonamos o elemento `ix` del dataset\n",
    "    def __getitem__(self, ix):\n",
    "        return self.X[ix], self.Y[ix]\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma vez definida a Classe, podemos instanciar um objeto que podemos usar para Iterar pelos nossos Dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset(X_train, y_train)\n",
    "\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 Loss 0.31284\n",
      "Epoch 2/30 Loss 0.22046\n",
      "Epoch 3/30 Loss 0.17761\n",
      "Epoch 4/30 Loss 0.15118\n",
      "Epoch 5/30 Loss 0.13265\n",
      "Epoch 6/30 Loss 0.11857\n",
      "Epoch 7/30 Loss 0.10724\n",
      "Epoch 8/30 Loss 0.09780\n",
      "Epoch 9/30 Loss 0.08982\n",
      "Epoch 10/30 Loss 0.08291\n",
      "Epoch 11/30 Loss 0.07695\n",
      "Epoch 12/30 Loss 0.07172\n",
      "Epoch 13/30 Loss 0.06717\n",
      "Epoch 14/30 Loss 0.06305\n",
      "Epoch 15/30 Loss 0.05936\n",
      "Epoch 16/30 Loss 0.05604\n",
      "Epoch 17/30 Loss 0.05305\n",
      "Epoch 18/30 Loss 0.05034\n",
      "Epoch 19/30 Loss 0.04787\n",
      "Epoch 20/30 Loss 0.04562\n",
      "Epoch 21/30 Loss 0.04358\n",
      "Epoch 22/30 Loss 0.04170\n",
      "Epoch 23/30 Loss 0.03997\n",
      "Epoch 24/30 Loss 0.03838\n",
      "Epoch 25/30 Loss 0.03691\n",
      "Epoch 26/30 Loss 0.03554\n",
      "Epoch 27/30 Loss 0.03428\n",
      "Epoch 28/30 Loss 0.03310\n",
      "Epoch 29/30 Loss 0.03199\n",
      "Epoch 30/30 Loss 0.03096\n",
      "\n",
      "\u001b[93mA accuracy é: \u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9767"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_in, H, D_out = 784, 100, 10\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ").to(\"cuda\")\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.8)\n",
    "\n",
    "epochs = 30\n",
    "batch_size = 100\n",
    "log_each = 1\n",
    "l = []\n",
    "model.train()\n",
    "batches = len(dataset) // batch_size\n",
    "for e in range(1, epochs+1): \n",
    "    \n",
    "    _l = []\n",
    "    # iteramos por batches en el dataset\n",
    "    for b in range(batches):\n",
    "        x_b, y_b = dataset[b*batch_size:(b+1)*batch_size]\n",
    "        \n",
    "        # forward\n",
    "        y_pred = model(x_b)\n",
    "\n",
    "        # loss\n",
    "        loss = criterion(y_pred, y_b)\n",
    "        _l.append(loss.item())\n",
    "\n",
    "        # ponemos a cero los gradientes\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backprop (calculamos todos los gradientes automáticamente)\n",
    "        loss.backward()\n",
    "\n",
    "        # update de los pesos\n",
    "        optimizer.step()\n",
    "    \n",
    "    l.append(np.mean(_l))\n",
    "    if not e % log_each:\n",
    "        print(f\"Epoch {e}/{epochs} Loss {np.mean(l):.5f}\")\n",
    "        \n",
    "y_pred = evaluate(torch.from_numpy(X_test.values).float().cuda())\n",
    "\n",
    "\n",
    "print(\"\")\n",
    "print(\"\\033[93mA accuracy é: \\033[0m\")\n",
    "accuracy_score(y_test, y_pred.cpu().numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como você pode ver, podemos ITERAR diretamente sobre o objeto `dataset` da mesma maneira que fizemos anteriormente, no entanto `Pytorch` não oferece outro objeto que nos facilte as coisas na hora de Iterar por bacthes. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Classe DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LightningAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
