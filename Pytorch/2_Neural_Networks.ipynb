{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"><font color=\"yellow\">Pytorch: Neural Networks</font></h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"yellow\">Data Scientist.: PhD.Eddy Giusepe Chirinos Isidro</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.9.13\n",
      "IPython version      : 8.13.2\n",
      "\n",
      "numpy     : 1.24.3\n",
      "pandas    : 2.0.1\n",
      "matplotlib: 3.7.1\n",
      "requests  : 2.31.0\n",
      "torch     : 2.0.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark \n",
    "%watermark -v -p numpy,pandas,matplotlib,requests,torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Modelos Sequenciais</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A forma mais simples de definir uma `Rede Neural` em `Pytorch` é utilizando a classe `Sequential`. Esta classe nos permite definir uma sequência de camadas, que se aplicaram de maneira sequencial (as saídas de uma camada serão a entrada da seguinte). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_in, H, D_out = 784, 100, 10\n",
    "\n",
    "model = torch.nn.Sequential(torch.nn.Linear(D_in, H), # H --> Hidden\n",
    "                            torch.nn.ReLU(),\n",
    "                            torch.nn.Linear(H, D_out),\n",
    "                           )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este modelo anterior é um `MLP` com $784$ entradas, $100$ neurônios na camada oculta e $10$ saídas. Vejamos um exemplo de como calcular as saídas do Modelo a partir de umas entradas de exemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(torch.randn(64, 784))\n",
    "outputs.shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">É importante observar que os modelos de `Pytorch` (pelo geral) sempre esperam que a primeira dimensão seja a `Dimensão Batch`. Lembramos que treinar na `GPU` é assim:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=784, out_features=100, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=100, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cuda\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pegamos como exemplo: MNIST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eddygiusepe/miniconda3/envs/LightningAI/lib/python3.9/site-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((70000, 784), (70000,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# descarga datos\n",
    "\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "X, Y = mnist[\"data\"], mnist[\"target\"]\n",
    "\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Normalização e Split:\n",
    "\n",
    "X_train, X_test, y_train, y_test = X[:60000] / 255., X[60000:] / 255., Y[:60000].astype(int), Y[60000:].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função Loss e Derivada:\n",
    "\n",
    "def softmax(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(axis=-1,keepdims=True)\n",
    "\n",
    "\n",
    "def cross_entropy(output, target):\n",
    "    logits = output[torch.arange(len(output)), target]\n",
    "    loss = - logits + torch.log(torch.sum(torch.exp(output), axis=-1))\n",
    "    loss = loss.mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/1000 Loss 0.24230\n",
      "Epoch 20/1000 Loss 0.23686\n",
      "Epoch 30/1000 Loss 0.23204\n",
      "Epoch 40/1000 Loss 0.22762\n",
      "Epoch 50/1000 Loss 0.22352\n",
      "Epoch 60/1000 Loss 0.21967\n",
      "Epoch 70/1000 Loss 0.21605\n",
      "Epoch 80/1000 Loss 0.21263\n",
      "Epoch 90/1000 Loss 0.20938\n",
      "Epoch 100/1000 Loss 0.20629\n",
      "Epoch 110/1000 Loss 0.20334\n",
      "Epoch 120/1000 Loss 0.20052\n",
      "Epoch 130/1000 Loss 0.19781\n",
      "Epoch 140/1000 Loss 0.19522\n",
      "Epoch 150/1000 Loss 0.19273\n",
      "Epoch 160/1000 Loss 0.19033\n",
      "Epoch 170/1000 Loss 0.18801\n",
      "Epoch 180/1000 Loss 0.18578\n",
      "Epoch 190/1000 Loss 0.18363\n",
      "Epoch 200/1000 Loss 0.18154\n",
      "Epoch 210/1000 Loss 0.17953\n",
      "Epoch 220/1000 Loss 0.17757\n",
      "Epoch 230/1000 Loss 0.17568\n",
      "Epoch 240/1000 Loss 0.17383\n",
      "Epoch 250/1000 Loss 0.17204\n",
      "Epoch 260/1000 Loss 0.17031\n",
      "Epoch 270/1000 Loss 0.16861\n",
      "Epoch 280/1000 Loss 0.16697\n",
      "Epoch 290/1000 Loss 0.16536\n",
      "Epoch 300/1000 Loss 0.16380\n",
      "Epoch 310/1000 Loss 0.16227\n",
      "Epoch 320/1000 Loss 0.16078\n",
      "Epoch 330/1000 Loss 0.15933\n",
      "Epoch 340/1000 Loss 0.15791\n",
      "Epoch 350/1000 Loss 0.15652\n",
      "Epoch 360/1000 Loss 0.15516\n",
      "Epoch 370/1000 Loss 0.15383\n",
      "Epoch 380/1000 Loss 0.15253\n",
      "Epoch 390/1000 Loss 0.15126\n",
      "Epoch 400/1000 Loss 0.15002\n",
      "Epoch 410/1000 Loss 0.14880\n",
      "Epoch 420/1000 Loss 0.14761\n",
      "Epoch 430/1000 Loss 0.14644\n",
      "Epoch 440/1000 Loss 0.14529\n",
      "Epoch 450/1000 Loss 0.14417\n",
      "Epoch 460/1000 Loss 0.14306\n",
      "Epoch 470/1000 Loss 0.14198\n",
      "Epoch 480/1000 Loss 0.14092\n",
      "Epoch 490/1000 Loss 0.13988\n",
      "Epoch 500/1000 Loss 0.13885\n",
      "Epoch 510/1000 Loss 0.13785\n",
      "Epoch 520/1000 Loss 0.13686\n",
      "Epoch 530/1000 Loss 0.13589\n",
      "Epoch 540/1000 Loss 0.13494\n",
      "Epoch 550/1000 Loss 0.13400\n",
      "Epoch 560/1000 Loss 0.13308\n",
      "Epoch 570/1000 Loss 0.13217\n",
      "Epoch 580/1000 Loss 0.13128\n",
      "Epoch 590/1000 Loss 0.13040\n",
      "Epoch 600/1000 Loss 0.12954\n",
      "Epoch 610/1000 Loss 0.12869\n",
      "Epoch 620/1000 Loss 0.12786\n",
      "Epoch 630/1000 Loss 0.12704\n",
      "Epoch 640/1000 Loss 0.12623\n",
      "Epoch 650/1000 Loss 0.12543\n",
      "Epoch 660/1000 Loss 0.12465\n",
      "Epoch 670/1000 Loss 0.12387\n",
      "Epoch 680/1000 Loss 0.12311\n",
      "Epoch 690/1000 Loss 0.12236\n",
      "Epoch 700/1000 Loss 0.12162\n",
      "Epoch 710/1000 Loss 0.12089\n",
      "Epoch 720/1000 Loss 0.12017\n",
      "Epoch 730/1000 Loss 0.11946\n",
      "Epoch 740/1000 Loss 0.11876\n",
      "Epoch 750/1000 Loss 0.11807\n",
      "Epoch 760/1000 Loss 0.11739\n",
      "Epoch 770/1000 Loss 0.11672\n",
      "Epoch 780/1000 Loss 0.11606\n",
      "Epoch 790/1000 Loss 0.11541\n",
      "Epoch 800/1000 Loss 0.11476\n",
      "Epoch 810/1000 Loss 0.11413\n",
      "Epoch 820/1000 Loss 0.11350\n",
      "Epoch 830/1000 Loss 0.11288\n",
      "Epoch 840/1000 Loss 0.11226\n",
      "Epoch 850/1000 Loss 0.11166\n",
      "Epoch 860/1000 Loss 0.11106\n",
      "Epoch 870/1000 Loss 0.11047\n",
      "Epoch 880/1000 Loss 0.10989\n",
      "Epoch 890/1000 Loss 0.10931\n",
      "Epoch 900/1000 Loss 0.10874\n",
      "Epoch 910/1000 Loss 0.10818\n",
      "Epoch 920/1000 Loss 0.10763\n",
      "Epoch 930/1000 Loss 0.10708\n",
      "Epoch 940/1000 Loss 0.10653\n",
      "Epoch 950/1000 Loss 0.10600\n",
      "Epoch 960/1000 Loss 0.10547\n",
      "Epoch 970/1000 Loss 0.10494\n",
      "Epoch 980/1000 Loss 0.10443\n",
      "Epoch 990/1000 Loss 0.10391\n",
      "Epoch 1000/1000 Loss 0.10341\n"
     ]
    }
   ],
   "source": [
    "# Convertemos os dados a tensores e copiamos para a  gpu\n",
    "X_t = torch.from_numpy(X_train.values).float().cuda()\n",
    "Y_t = torch.from_numpy(y_train.values).long().cuda()\n",
    "\n",
    "# Loop de Treinamento\n",
    "epochs = 1000\n",
    "lr = 0.8\n",
    "log_each = 10\n",
    "l = []\n",
    "for e in range(1, epochs+1): \n",
    "    \n",
    "    # forward\n",
    "    y_pred = model(X_t)\n",
    "\n",
    "    # loss\n",
    "    loss = cross_entropy(y_pred, Y_t)\n",
    "    l.append(loss.item())\n",
    "    \n",
    "    # Zeramos os gradientes:\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Backprop (calculamos todos os gradientes automáticamente)\n",
    "    loss.backward()\n",
    "\n",
    "    # update dos pesos:\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= lr * param.grad\n",
    "    \n",
    "    if not e % log_each:\n",
    "        print(f\"Epoch {e}/{epochs} Loss {np.mean(l):.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9745"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def evaluate(x):\n",
    "    model.eval()\n",
    "    y_pred = model(x)\n",
    "    y_probas = softmax(y_pred)\n",
    "    return torch.argmax(y_probas, axis=1)\n",
    "\n",
    "y_pred = evaluate(torch.from_numpy(X_test.values).float().cuda())\n",
    "accuracy_score(y_test, y_pred.cpu().numpy())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Otimizadores e Funções de Perda</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos usar as funções que Pytorch nos facilita. Ver a [Documentação Pytorch](https://pytorch.org/docs/stable/index.html) para mais detalhes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Otimizadores em `torch.optim`\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nosso Loop de Treinamento fica mais compactado, assim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100 Loss 1.78106\n",
      "Epoch 20/100 Loss 1.36611\n",
      "Epoch 30/100 Loss 1.12115\n",
      "Epoch 40/100 Loss 0.97920\n",
      "Epoch 50/100 Loss 0.86094\n",
      "Epoch 60/100 Loss 0.77130\n",
      "Epoch 70/100 Loss 0.70736\n",
      "Epoch 80/100 Loss 0.65396\n",
      "Epoch 90/100 Loss 0.61035\n",
      "Epoch 100/100 Loss 0.57421\n",
      "\n",
      "\u001b[93mA accuracy é: \u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9341"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.nn.Sequential(torch.nn.Linear(D_in, H),\n",
    "                            torch.nn.ReLU(),\n",
    "                            torch.nn.Linear(H, D_out),\n",
    "                           ).to(\"cuda\")\n",
    "\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.8)\n",
    "\n",
    "epochs = 100\n",
    "log_each = 10\n",
    "l = []\n",
    "model.train()\n",
    "for e in range(1, epochs+1): \n",
    "    \n",
    "    # forward\n",
    "    y_pred = model(X_t)\n",
    "\n",
    "    # loss\n",
    "    loss = criterion(y_pred, Y_t)\n",
    "    l.append(loss.item())\n",
    "    \n",
    "    # Zeramos os gradientes\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backprop (calculamos todos os gradientes automáticamente)\n",
    "    loss.backward()\n",
    "\n",
    "    # update dos pesos\n",
    "    optimizer.step()\n",
    "    \n",
    "    if not e % log_each:\n",
    "        print(f\"Epoch {e}/{epochs} Loss {np.mean(l):.5f}\")\n",
    "        \n",
    "y_pred = evaluate(torch.from_numpy(X_test.values).float().cuda())\n",
    "\n",
    "print(\"\")\n",
    "print(\"\\033[93mA accuracy é: \\033[0m\")\n",
    "accuracy_score(y_test, y_pred.cpu().numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"pink\">Modelos customizados</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Em muitos casos definir uma `Rede Neural` como uma sequência de camadas é suficiente, em outros casos será um fator limitante. `Um exemplo` são as Redes Residuais, nas que não só utilizamos a saída de uma camada para alimentar a seguinte senão que, ademais, sumamos sua própria entrada. Esse tipo de arquitetura não pode ser definida com a classe `Sequential`, para isso precisamos CUSTOMIZAR. Para isso `Pytorch` nos oferece a seguinte sintaxe:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criamos uma classe que herda de `torch.nn.Module`\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    \n",
    "    # Construtor\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \n",
    "        # Chamamos ao construtor da classe pai\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        # Definimos nossa camadas\n",
    "        self.fc1 = torch.nn.Linear(D_in, H)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.fc2 = torch.nn.Linear(H, D_out)\n",
    "        \n",
    "    # Lógica para calcular as saídas da Rede:\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(784, 100, 10)\n",
    "\n",
    "outputs = model(torch.randn(64, 784))\n",
    "outputs.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Agora, podemos treinar a nossa Rede Neural da mesma forma que fizemos anteriormente:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/500 Loss 0.24836\n",
      "Epoch 20/500 Loss 0.24610\n",
      "Epoch 30/500 Loss 0.38185\n",
      "Epoch 40/500 Loss 0.35002\n",
      "Epoch 50/500 Loss 0.32750\n",
      "Epoch 60/500 Loss 0.31121\n",
      "Epoch 70/500 Loss 0.29875\n",
      "Epoch 80/500 Loss 0.28878\n",
      "Epoch 90/500 Loss 0.28052\n",
      "Epoch 100/500 Loss 0.27350\n",
      "Epoch 110/500 Loss 0.26739\n",
      "Epoch 120/500 Loss 0.26199\n",
      "Epoch 130/500 Loss 0.25715\n",
      "Epoch 140/500 Loss 0.25275\n",
      "Epoch 150/500 Loss 0.24872\n",
      "Epoch 160/500 Loss 0.24499\n",
      "Epoch 170/500 Loss 0.24153\n",
      "Epoch 180/500 Loss 0.23828\n",
      "Epoch 190/500 Loss 0.23522\n",
      "Epoch 200/500 Loss 0.23232\n",
      "Epoch 210/500 Loss 0.22958\n",
      "Epoch 220/500 Loss 0.22696\n",
      "Epoch 230/500 Loss 0.22446\n",
      "Epoch 240/500 Loss 0.22206\n",
      "Epoch 250/500 Loss 0.21976\n",
      "Epoch 260/500 Loss 0.21754\n",
      "Epoch 270/500 Loss 0.21541\n",
      "Epoch 280/500 Loss 0.21335\n",
      "Epoch 290/500 Loss 0.21135\n",
      "Epoch 300/500 Loss 0.20942\n",
      "Epoch 310/500 Loss 0.20755\n",
      "Epoch 320/500 Loss 0.20573\n",
      "Epoch 330/500 Loss 0.20397\n",
      "Epoch 340/500 Loss 0.20225\n",
      "Epoch 350/500 Loss 0.20058\n",
      "Epoch 360/500 Loss 0.19896\n",
      "Epoch 370/500 Loss 0.19737\n",
      "Epoch 380/500 Loss 0.19582\n",
      "Epoch 390/500 Loss 0.19432\n",
      "Epoch 400/500 Loss 0.19284\n",
      "Epoch 410/500 Loss 0.19140\n",
      "Epoch 420/500 Loss 0.18999\n",
      "Epoch 430/500 Loss 0.18862\n",
      "Epoch 440/500 Loss 0.18727\n",
      "Epoch 450/500 Loss 0.18595\n",
      "Epoch 460/500 Loss 0.18466\n",
      "Epoch 470/500 Loss 0.18339\n",
      "Epoch 480/500 Loss 0.18215\n",
      "Epoch 490/500 Loss 0.18093\n",
      "Epoch 500/500 Loss 0.17974\n",
      "\n",
      "\u001b[93mA accuracy é: \u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.963"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cuda\")\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.5)\n",
    "\n",
    "epochs = 500\n",
    "log_each = 10\n",
    "l = []\n",
    "model.train()\n",
    "for e in range(1, epochs+1): \n",
    "    \n",
    "    # Função Forward\n",
    "    y_pred = model(X_t)\n",
    "\n",
    "    # loss\n",
    "    loss = criterion(y_pred, Y_t)\n",
    "    l.append(loss.item())\n",
    "    \n",
    "    # ponemos a cero los gradientes\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backprop (calculamos todos los gradientes automáticamente)\n",
    "    loss.backward()\n",
    "\n",
    "    # update de los pesos\n",
    "    optimizer.step()\n",
    "    \n",
    "    if not e % log_each:\n",
    "        print(f\"Epoch {e}/{epochs} Loss {np.mean(l):.5f}\")\n",
    "        \n",
    "y_pred = evaluate(torch.from_numpy(X_test.values).float().cuda())\n",
    "print(\"\")\n",
    "\n",
    "print(\"\\033[93mA accuracy é: \\033[0m\")\n",
    "accuracy_score(y_test, y_pred.cpu().numpy())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">A seguir vamos ver outro exemplo de como definir `MLP` com conexões residuais, algo que podemos fazer simplesmente usando um Modelo sequential:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LightningAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
