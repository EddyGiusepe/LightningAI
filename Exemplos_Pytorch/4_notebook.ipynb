{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"><font color=\"yellow\">nn module - Pytorch: nn</font></h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"yellow\">Data Scientist.: PhD.Eddy Giusepe Chirinos Isidro</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "nn module - Pytorch: nn\n",
    "=======================\n",
    "\n",
    "No PyTorch, o pacote nn serve para esse mesmo propósito (nível alto de Grafos computacionais brutos que são úteis para construir Redes Neurais). \n",
    "O pacote nn define um conjunto de Módulos, que são aproximadamente equivalentes a camadas de rede neural. Um módulo recebe tensores de entrada \n",
    "e calcula tensores de saída, mas também pode conter estado interno, como tensores contendo parâmetros que podem ser aprendidos. O pacote nn também \n",
    "define um conjunto de funções de perda úteis que são comumente usadas ao treinar redes neurais.\n",
    "\n",
    "Link de estudo ---> https://pytorch.org/tutorials/beginner/pytorch_with_examples.html\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "# Crie tensores para armazenar entradas e saídas:\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este exemplo, a saída $y$ é uma função linear de $(x, x^2, x^3)$, então podemos considerá-la como uma rede neural de camada linear. Vamos preparar o tensor $(x, x^2, x^3)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = torch.tensor([1, 2, 3])\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -3.1416,   9.8696, -31.0063],\n",
       "        [ -3.1384,   9.8499, -30.9133],\n",
       "        [ -3.1353,   9.8301, -30.8205],\n",
       "        ...,\n",
       "        [  3.1353,   9.8301,  30.8205],\n",
       "        [  3.1384,   9.8499,  30.9133],\n",
       "        [  3.1416,   9.8696,  31.0063]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x.unsqueeze(-1) --> Adiciona uma dimensão extra ao tensor 'x':\n",
    "xx = x.unsqueeze(-1).pow(p)\n",
    "\n",
    "xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2000, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No código acima, `x.unsqueeze(-1)` tem forma $(2000, 1)$ e $p$ tem forma $(3,)$, para este caso, a semântica de transmissão será aplicada para obter um tensor de forma $(2000, 3)$.\n",
    "\n",
    "\n",
    "\n",
    "Use o pacote `nn` para definir nosso modelo como uma `sequência de camadas`. `nn.Sequential` é um Módulo que contém outros Módulos, e os aplica em seqüência para produzir sua saída. O Módulo Linear calcula a saída da entrada usando uma função linear e mantém os Tensores internos para seu peso e viés. A `camada Flatten` nivela a saída da camada linear para um `tensor 1D`, para corresponder à forma de `y`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 1),\n",
    "    torch.nn.Flatten(0, 1)\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O pacote `nn` também contém definições de funções de perda populares; neste caso, usaremos o `Mean Squared Error` (`MSE`) como nossa função de perda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MSELoss()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 1166.9737548828125\n",
      "199 785.9876098632812\n",
      "299 530.7454833984375\n",
      "399 359.6208190917969\n",
      "499 244.80453491210938\n",
      "599 167.70742797851562\n",
      "699 115.89563751220703\n",
      "799 81.04689025878906\n",
      "899 57.58669662475586\n",
      "999 41.7790641784668\n",
      "1099 31.11776351928711\n",
      "1199 23.92055320739746\n",
      "1299 19.05698585510254\n",
      "1399 15.76708984375\n",
      "1499 13.539349555969238\n",
      "1599 12.02926254272461\n",
      "1699 11.004531860351562\n",
      "1799 10.308424949645996\n",
      "1899 9.834992408752441\n",
      "1999 9.51266098022461\n"
     ]
    }
   ],
   "source": [
    "for t in range(2000):\n",
    "\n",
    "    \"\"\"\n",
    "    Forward pass: calcula 'y' previsto passando x para o modelo. \n",
    "    Os objetos do módulo substituem o operador __call__ para que você \n",
    "    possa chamá-los como funções. Ao fazer isso, você passa um tensor de \n",
    "    dados de entrada para o módulo e produz um tensor de dados de saída.\n",
    "    \"\"\"\n",
    "    y_pred = model(xx)\n",
    "\n",
    "    \"\"\"\n",
    "    Calculamos e printamos a loss. Passamos Tensores contendo os valores previstos e \n",
    "    verdadeiros de 'y', e a função de perda retorna um Tensor contendo a perda.\n",
    "    \"\"\"\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zere os gradientes antes de executar a passagem para trás (backward).\n",
    "    model.zero_grad()\n",
    "\n",
    "    \"\"\"\n",
    "    Backward pass: calcula o gradiente da Loss em relação a todos os parâmetros apreensíveis do modelo. \n",
    "    Internamente, os parâmetros de cada Módulo são armazenados em Tensores com 'require_grad=True', então \n",
    "    esta chamada calculará gradientes para todos os parâmetros que podem ser aprendidos no modelo.\n",
    "    \"\"\"\n",
    "    loss.backward()\n",
    "\n",
    "    \"\"\"\n",
    "    Atualize os pesos usando gradiente descendente. Cada parâmetro é um Tensor, então podemos acessar seus gradientes como fizemos antes.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=3, out_features=1, bias=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Você pode acessar a primeira camada do 'model' como acessar o primeiro item de uma lista:\n",
    "linear_layer = model[0]\n",
    "linear_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Flatten(start_dim=0, end_dim=1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer_1 = model[1]\n",
    "linear_layer_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: y = 0.019588246941566467 + 0.838476300239563 x + -0.0033792981412261724 x^2 + -0.09073241800069809 x^3\n"
     ]
    }
   ],
   "source": [
    "# Para a camada linear, seus parâmetros são armazenados como \"weight\" e \"bias'.\n",
    "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LightningAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
