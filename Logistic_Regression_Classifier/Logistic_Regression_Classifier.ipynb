{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"><font color=\"yellow\">Logistic Regression Classifier with Pytorch</font></h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"yellow\">Data Scientist.: PhD.Eddy Giusepe Chirinos Isidro</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The watermark extension is already loaded. To reload it, use:\n",
      "  %reload_ext watermark\n",
      "Python implementation: CPython\n",
      "Python version       : 3.9.13\n",
      "IPython version      : 8.13.2\n",
      "\n",
      "numpy     : 1.24.3\n",
      "pandas    : 2.0.1\n",
      "matplotlib: 3.7.1\n",
      "torch     : 2.0.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verificamos as bibliotecas instaladas\n",
    "\n",
    "%load_ext watermark \n",
    "%watermark -v -p numpy,pandas,matplotlib,torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nosso Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.77</td>\n",
       "      <td>-1.14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.33</td>\n",
       "      <td>1.44</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.91</td>\n",
       "      <td>-3.07</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.37</td>\n",
       "      <td>-1.91</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.63</td>\n",
       "      <td>-1.53</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     x1    x2  label\n",
       "0  0.77 -1.14      0\n",
       "1 -0.33  1.44      0\n",
       "2  0.91 -3.07      0\n",
       "3 -0.37 -1.91      0\n",
       "4 -0.63 -1.53      0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv('../perceptron/perceptron_toydata-truncated.txt', sep='\\t')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df[[\"x1\", \"x2\"]].values\n",
    "y_train = df[\"label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.77, -1.14],\n",
       "       [-0.33,  1.44],\n",
       "       [ 0.91, -3.07],\n",
       "       [-0.37, -1.91],\n",
       "       [-0.63, -1.53],\n",
       "       [ 0.39, -1.99],\n",
       "       [-0.49, -2.74],\n",
       "       [-0.68, -1.52],\n",
       "       [-0.1 , -3.43],\n",
       "       [-0.05, -1.95],\n",
       "       [ 3.88,  0.65],\n",
       "       [ 0.73,  2.97],\n",
       "       [ 0.83,  3.94],\n",
       "       [ 1.59,  1.25],\n",
       "       [ 1.14,  3.91],\n",
       "       [ 1.73,  2.8 ],\n",
       "       [ 1.31,  1.85],\n",
       "       [ 1.56,  3.85],\n",
       "       [ 1.23,  2.54],\n",
       "       [ 1.33,  2.03]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7375, 0.3975])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.mean(axis=0) # 0 --> Coluna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = (X_train - X_train.mean(axis=0)) / X_train.std(axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0305863 , -0.61935683],\n",
       "       [-1.00464248,  0.41995414],\n",
       "       [ 0.1623427 , -1.39682589],\n",
       "       [-1.04228716, -0.92953879],\n",
       "       [-1.2869776 , -0.77646198],\n",
       "       [-0.32703818, -0.96176548],\n",
       "       [-1.15522121, -1.26389077],\n",
       "       [-1.33403345, -0.77243364],\n",
       "       [-0.78818555, -1.54184603],\n",
       "       [-0.7411297 , -0.94565213],\n",
       "       [ 2.95746041,  0.10171551],\n",
       "       [-0.00705838,  1.03628972],\n",
       "       [ 0.08705333,  1.42703842],\n",
       "       [ 0.80230231,  0.34341574],\n",
       "       [ 0.37879962,  1.41495341],\n",
       "       [ 0.9340587 ,  0.96780799],\n",
       "       [ 0.53878953,  0.58511596],\n",
       "       [ 0.77406879,  1.39078338],\n",
       "       [ 0.46350016,  0.86307122],\n",
       "       [ 0.55761187,  0.65762603]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementamos o Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class LogisticRegression(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(num_features, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear(x)\n",
    "        probas = torch.sigmoid(logits)\n",
    "        return probas   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "\n",
    "model = LogisticRegression(num_features=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4033])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.1, 2.1])\n",
    "\n",
    "with torch.no_grad():\n",
    "    proba = model(x)\n",
    "    \n",
    "print(proba)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definindo nosso Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        \"\"\"X --> Tensor features e y --> Tensor Label\"\"\"\n",
    "\n",
    "        self.features = torch.tensor(X, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.features[index]\n",
    "        y = self.labels[index]        \n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]\n",
    "    \n",
    "\n",
    "\n",
    "train_ds = MyDataset(X_train, y_train)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(dataset=train_ds,\n",
    "                          batch_size=10,\n",
    "                          shuffle=True,\n",
    "                          num_workers=8 # A minha m√°quina tem 8 Workers\n",
    "                         )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# O Loop de Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/200 | Batch 000/002 | Loss: 0.67\n",
      "Epoch: 001/200 | Batch 001/002 | Loss: 0.73\n",
      "Epoch: 002/200 | Batch 000/002 | Loss: 0.67\n",
      "Epoch: 002/200 | Batch 001/002 | Loss: 0.67\n",
      "Epoch: 003/200 | Batch 000/002 | Loss: 0.60\n",
      "Epoch: 003/200 | Batch 001/002 | Loss: 0.68\n",
      "Epoch: 004/200 | Batch 000/002 | Loss: 0.69\n",
      "Epoch: 004/200 | Batch 001/002 | Loss: 0.54\n",
      "Epoch: 005/200 | Batch 000/002 | Loss: 0.61\n",
      "Epoch: 005/200 | Batch 001/002 | Loss: 0.57\n",
      "Epoch: 006/200 | Batch 000/002 | Loss: 0.59\n",
      "Epoch: 006/200 | Batch 001/002 | Loss: 0.54\n",
      "Epoch: 007/200 | Batch 000/002 | Loss: 0.51\n",
      "Epoch: 007/200 | Batch 001/002 | Loss: 0.58\n",
      "Epoch: 008/200 | Batch 000/002 | Loss: 0.51\n",
      "Epoch: 008/200 | Batch 001/002 | Loss: 0.54\n",
      "Epoch: 009/200 | Batch 000/002 | Loss: 0.51\n",
      "Epoch: 009/200 | Batch 001/002 | Loss: 0.49\n",
      "Epoch: 010/200 | Batch 000/002 | Loss: 0.53\n",
      "Epoch: 010/200 | Batch 001/002 | Loss: 0.44\n",
      "Epoch: 011/200 | Batch 000/002 | Loss: 0.42\n",
      "Epoch: 011/200 | Batch 001/002 | Loss: 0.52\n",
      "Epoch: 012/200 | Batch 000/002 | Loss: 0.46\n",
      "Epoch: 012/200 | Batch 001/002 | Loss: 0.44\n",
      "Epoch: 013/200 | Batch 000/002 | Loss: 0.46\n",
      "Epoch: 013/200 | Batch 001/002 | Loss: 0.42\n",
      "Epoch: 014/200 | Batch 000/002 | Loss: 0.41\n",
      "Epoch: 014/200 | Batch 001/002 | Loss: 0.43\n",
      "Epoch: 015/200 | Batch 000/002 | Loss: 0.43\n",
      "Epoch: 015/200 | Batch 001/002 | Loss: 0.39\n",
      "Epoch: 016/200 | Batch 000/002 | Loss: 0.41\n",
      "Epoch: 016/200 | Batch 001/002 | Loss: 0.39\n",
      "Epoch: 017/200 | Batch 000/002 | Loss: 0.38\n",
      "Epoch: 017/200 | Batch 001/002 | Loss: 0.39\n",
      "Epoch: 018/200 | Batch 000/002 | Loss: 0.44\n",
      "Epoch: 018/200 | Batch 001/002 | Loss: 0.31\n",
      "Epoch: 019/200 | Batch 000/002 | Loss: 0.34\n",
      "Epoch: 019/200 | Batch 001/002 | Loss: 0.39\n",
      "Epoch: 020/200 | Batch 000/002 | Loss: 0.33\n",
      "Epoch: 020/200 | Batch 001/002 | Loss: 0.38\n",
      "Epoch: 021/200 | Batch 000/002 | Loss: 0.39\n",
      "Epoch: 021/200 | Batch 001/002 | Loss: 0.30\n",
      "Epoch: 022/200 | Batch 000/002 | Loss: 0.34\n",
      "Epoch: 022/200 | Batch 001/002 | Loss: 0.33\n",
      "Epoch: 023/200 | Batch 000/002 | Loss: 0.37\n",
      "Epoch: 023/200 | Batch 001/002 | Loss: 0.29\n",
      "Epoch: 024/200 | Batch 000/002 | Loss: 0.38\n",
      "Epoch: 024/200 | Batch 001/002 | Loss: 0.26\n",
      "Epoch: 025/200 | Batch 000/002 | Loss: 0.32\n",
      "Epoch: 025/200 | Batch 001/002 | Loss: 0.31\n",
      "Epoch: 026/200 | Batch 000/002 | Loss: 0.32\n",
      "Epoch: 026/200 | Batch 001/002 | Loss: 0.29\n",
      "Epoch: 027/200 | Batch 000/002 | Loss: 0.33\n",
      "Epoch: 027/200 | Batch 001/002 | Loss: 0.26\n",
      "Epoch: 028/200 | Batch 000/002 | Loss: 0.31\n",
      "Epoch: 028/200 | Batch 001/002 | Loss: 0.27\n",
      "Epoch: 029/200 | Batch 000/002 | Loss: 0.33\n",
      "Epoch: 029/200 | Batch 001/002 | Loss: 0.24\n",
      "Epoch: 030/200 | Batch 000/002 | Loss: 0.34\n",
      "Epoch: 030/200 | Batch 001/002 | Loss: 0.21\n",
      "Epoch: 031/200 | Batch 000/002 | Loss: 0.30\n",
      "Epoch: 031/200 | Batch 001/002 | Loss: 0.25\n",
      "Epoch: 032/200 | Batch 000/002 | Loss: 0.29\n",
      "Epoch: 032/200 | Batch 001/002 | Loss: 0.24\n",
      "Epoch: 033/200 | Batch 000/002 | Loss: 0.24\n",
      "Epoch: 033/200 | Batch 001/002 | Loss: 0.28\n",
      "Epoch: 034/200 | Batch 000/002 | Loss: 0.25\n",
      "Epoch: 034/200 | Batch 001/002 | Loss: 0.26\n",
      "Epoch: 035/200 | Batch 000/002 | Loss: 0.19\n",
      "Epoch: 035/200 | Batch 001/002 | Loss: 0.31\n",
      "Epoch: 036/200 | Batch 000/002 | Loss: 0.22\n",
      "Epoch: 036/200 | Batch 001/002 | Loss: 0.28\n",
      "Epoch: 037/200 | Batch 000/002 | Loss: 0.27\n",
      "Epoch: 037/200 | Batch 001/002 | Loss: 0.22\n",
      "Epoch: 038/200 | Batch 000/002 | Loss: 0.25\n",
      "Epoch: 038/200 | Batch 001/002 | Loss: 0.23\n",
      "Epoch: 039/200 | Batch 000/002 | Loss: 0.23\n",
      "Epoch: 039/200 | Batch 001/002 | Loss: 0.23\n",
      "Epoch: 040/200 | Batch 000/002 | Loss: 0.26\n",
      "Epoch: 040/200 | Batch 001/002 | Loss: 0.20\n",
      "Epoch: 041/200 | Batch 000/002 | Loss: 0.25\n",
      "Epoch: 041/200 | Batch 001/002 | Loss: 0.20\n",
      "Epoch: 042/200 | Batch 000/002 | Loss: 0.22\n",
      "Epoch: 042/200 | Batch 001/002 | Loss: 0.22\n",
      "Epoch: 043/200 | Batch 000/002 | Loss: 0.21\n",
      "Epoch: 043/200 | Batch 001/002 | Loss: 0.23\n",
      "Epoch: 044/200 | Batch 000/002 | Loss: 0.17\n",
      "Epoch: 044/200 | Batch 001/002 | Loss: 0.26\n",
      "Epoch: 045/200 | Batch 000/002 | Loss: 0.22\n",
      "Epoch: 045/200 | Batch 001/002 | Loss: 0.20\n",
      "Epoch: 046/200 | Batch 000/002 | Loss: 0.27\n",
      "Epoch: 046/200 | Batch 001/002 | Loss: 0.15\n",
      "Epoch: 047/200 | Batch 000/002 | Loss: 0.22\n",
      "Epoch: 047/200 | Batch 001/002 | Loss: 0.19\n",
      "Epoch: 048/200 | Batch 000/002 | Loss: 0.24\n",
      "Epoch: 048/200 | Batch 001/002 | Loss: 0.17\n",
      "Epoch: 049/200 | Batch 000/002 | Loss: 0.21\n",
      "Epoch: 049/200 | Batch 001/002 | Loss: 0.19\n",
      "Epoch: 050/200 | Batch 000/002 | Loss: 0.19\n",
      "Epoch: 050/200 | Batch 001/002 | Loss: 0.20\n",
      "Epoch: 051/200 | Batch 000/002 | Loss: 0.19\n",
      "Epoch: 051/200 | Batch 001/002 | Loss: 0.20\n",
      "Epoch: 052/200 | Batch 000/002 | Loss: 0.16\n",
      "Epoch: 052/200 | Batch 001/002 | Loss: 0.22\n",
      "Epoch: 053/200 | Batch 000/002 | Loss: 0.22\n",
      "Epoch: 053/200 | Batch 001/002 | Loss: 0.15\n",
      "Epoch: 054/200 | Batch 000/002 | Loss: 0.21\n",
      "Epoch: 054/200 | Batch 001/002 | Loss: 0.16\n",
      "Epoch: 055/200 | Batch 000/002 | Loss: 0.17\n",
      "Epoch: 055/200 | Batch 001/002 | Loss: 0.20\n",
      "Epoch: 056/200 | Batch 000/002 | Loss: 0.17\n",
      "Epoch: 056/200 | Batch 001/002 | Loss: 0.20\n",
      "Epoch: 057/200 | Batch 000/002 | Loss: 0.20\n",
      "Epoch: 057/200 | Batch 001/002 | Loss: 0.16\n",
      "Epoch: 058/200 | Batch 000/002 | Loss: 0.17\n",
      "Epoch: 058/200 | Batch 001/002 | Loss: 0.18\n",
      "Epoch: 059/200 | Batch 000/002 | Loss: 0.22\n",
      "Epoch: 059/200 | Batch 001/002 | Loss: 0.13\n",
      "Epoch: 060/200 | Batch 000/002 | Loss: 0.13\n",
      "Epoch: 060/200 | Batch 001/002 | Loss: 0.21\n",
      "Epoch: 061/200 | Batch 000/002 | Loss: 0.20\n",
      "Epoch: 061/200 | Batch 001/002 | Loss: 0.14\n",
      "Epoch: 062/200 | Batch 000/002 | Loss: 0.16\n",
      "Epoch: 062/200 | Batch 001/002 | Loss: 0.17\n",
      "Epoch: 063/200 | Batch 000/002 | Loss: 0.14\n",
      "Epoch: 063/200 | Batch 001/002 | Loss: 0.19\n",
      "Epoch: 064/200 | Batch 000/002 | Loss: 0.13\n",
      "Epoch: 064/200 | Batch 001/002 | Loss: 0.20\n",
      "Epoch: 065/200 | Batch 000/002 | Loss: 0.15\n",
      "Epoch: 065/200 | Batch 001/002 | Loss: 0.18\n",
      "Epoch: 066/200 | Batch 000/002 | Loss: 0.21\n",
      "Epoch: 066/200 | Batch 001/002 | Loss: 0.11\n",
      "Epoch: 067/200 | Batch 000/002 | Loss: 0.16\n",
      "Epoch: 067/200 | Batch 001/002 | Loss: 0.16\n",
      "Epoch: 068/200 | Batch 000/002 | Loss: 0.19\n",
      "Epoch: 068/200 | Batch 001/002 | Loss: 0.12\n",
      "Epoch: 069/200 | Batch 000/002 | Loss: 0.11\n",
      "Epoch: 069/200 | Batch 001/002 | Loss: 0.20\n",
      "Epoch: 070/200 | Batch 000/002 | Loss: 0.12\n",
      "Epoch: 070/200 | Batch 001/002 | Loss: 0.19\n",
      "Epoch: 071/200 | Batch 000/002 | Loss: 0.15\n",
      "Epoch: 071/200 | Batch 001/002 | Loss: 0.15\n",
      "Epoch: 072/200 | Batch 000/002 | Loss: 0.14\n",
      "Epoch: 072/200 | Batch 001/002 | Loss: 0.16\n",
      "Epoch: 073/200 | Batch 000/002 | Loss: 0.17\n",
      "Epoch: 073/200 | Batch 001/002 | Loss: 0.13\n",
      "Epoch: 074/200 | Batch 000/002 | Loss: 0.11\n",
      "Epoch: 074/200 | Batch 001/002 | Loss: 0.19\n",
      "Epoch: 075/200 | Batch 000/002 | Loss: 0.12\n",
      "Epoch: 075/200 | Batch 001/002 | Loss: 0.17\n",
      "Epoch: 076/200 | Batch 000/002 | Loss: 0.14\n",
      "Epoch: 076/200 | Batch 001/002 | Loss: 0.15\n",
      "Epoch: 077/200 | Batch 000/002 | Loss: 0.18\n",
      "Epoch: 077/200 | Batch 001/002 | Loss: 0.10\n",
      "Epoch: 078/200 | Batch 000/002 | Loss: 0.15\n",
      "Epoch: 078/200 | Batch 001/002 | Loss: 0.14\n",
      "Epoch: 079/200 | Batch 000/002 | Loss: 0.17\n",
      "Epoch: 079/200 | Batch 001/002 | Loss: 0.11\n",
      "Epoch: 080/200 | Batch 000/002 | Loss: 0.14\n",
      "Epoch: 080/200 | Batch 001/002 | Loss: 0.14\n",
      "Epoch: 081/200 | Batch 000/002 | Loss: 0.18\n",
      "Epoch: 081/200 | Batch 001/002 | Loss: 0.10\n",
      "Epoch: 082/200 | Batch 000/002 | Loss: 0.13\n",
      "Epoch: 082/200 | Batch 001/002 | Loss: 0.14\n",
      "Epoch: 083/200 | Batch 000/002 | Loss: 0.13\n",
      "Epoch: 083/200 | Batch 001/002 | Loss: 0.14\n",
      "Epoch: 084/200 | Batch 000/002 | Loss: 0.17\n",
      "Epoch: 084/200 | Batch 001/002 | Loss: 0.10\n",
      "Epoch: 085/200 | Batch 000/002 | Loss: 0.18\n",
      "Epoch: 085/200 | Batch 001/002 | Loss: 0.09\n",
      "Epoch: 086/200 | Batch 000/002 | Loss: 0.10\n",
      "Epoch: 086/200 | Batch 001/002 | Loss: 0.17\n",
      "Epoch: 087/200 | Batch 000/002 | Loss: 0.15\n",
      "Epoch: 087/200 | Batch 001/002 | Loss: 0.11\n",
      "Epoch: 088/200 | Batch 000/002 | Loss: 0.12\n",
      "Epoch: 088/200 | Batch 001/002 | Loss: 0.14\n",
      "Epoch: 089/200 | Batch 000/002 | Loss: 0.14\n",
      "Epoch: 089/200 | Batch 001/002 | Loss: 0.12\n",
      "Epoch: 090/200 | Batch 000/002 | Loss: 0.12\n",
      "Epoch: 090/200 | Batch 001/002 | Loss: 0.14\n",
      "Epoch: 091/200 | Batch 000/002 | Loss: 0.12\n",
      "Epoch: 091/200 | Batch 001/002 | Loss: 0.14\n",
      "Epoch: 092/200 | Batch 000/002 | Loss: 0.08\n",
      "Epoch: 092/200 | Batch 001/002 | Loss: 0.17\n",
      "Epoch: 093/200 | Batch 000/002 | Loss: 0.10\n",
      "Epoch: 093/200 | Batch 001/002 | Loss: 0.15\n",
      "Epoch: 094/200 | Batch 000/002 | Loss: 0.13\n",
      "Epoch: 094/200 | Batch 001/002 | Loss: 0.12\n",
      "Epoch: 095/200 | Batch 000/002 | Loss: 0.14\n",
      "Epoch: 095/200 | Batch 001/002 | Loss: 0.11\n",
      "Epoch: 096/200 | Batch 000/002 | Loss: 0.11\n",
      "Epoch: 096/200 | Batch 001/002 | Loss: 0.13\n",
      "Epoch: 097/200 | Batch 000/002 | Loss: 0.12\n",
      "Epoch: 097/200 | Batch 001/002 | Loss: 0.12\n",
      "Epoch: 098/200 | Batch 000/002 | Loss: 0.13\n",
      "Epoch: 098/200 | Batch 001/002 | Loss: 0.11\n",
      "Epoch: 099/200 | Batch 000/002 | Loss: 0.10\n",
      "Epoch: 099/200 | Batch 001/002 | Loss: 0.13\n",
      "Epoch: 100/200 | Batch 000/002 | Loss: 0.10\n",
      "Epoch: 100/200 | Batch 001/002 | Loss: 0.13\n",
      "Epoch: 101/200 | Batch 000/002 | Loss: 0.13\n",
      "Epoch: 101/200 | Batch 001/002 | Loss: 0.11\n",
      "Epoch: 102/200 | Batch 000/002 | Loss: 0.08\n",
      "Epoch: 102/200 | Batch 001/002 | Loss: 0.16\n",
      "Epoch: 103/200 | Batch 000/002 | Loss: 0.10\n",
      "Epoch: 103/200 | Batch 001/002 | Loss: 0.13\n",
      "Epoch: 104/200 | Batch 000/002 | Loss: 0.13\n",
      "Epoch: 104/200 | Batch 001/002 | Loss: 0.10\n",
      "Epoch: 105/200 | Batch 000/002 | Loss: 0.12\n",
      "Epoch: 105/200 | Batch 001/002 | Loss: 0.11\n",
      "Epoch: 106/200 | Batch 000/002 | Loss: 0.10\n",
      "Epoch: 106/200 | Batch 001/002 | Loss: 0.12\n",
      "Epoch: 107/200 | Batch 000/002 | Loss: 0.11\n",
      "Epoch: 107/200 | Batch 001/002 | Loss: 0.11\n",
      "Epoch: 108/200 | Batch 000/002 | Loss: 0.07\n",
      "Epoch: 108/200 | Batch 001/002 | Loss: 0.15\n",
      "Epoch: 109/200 | Batch 000/002 | Loss: 0.10\n",
      "Epoch: 109/200 | Batch 001/002 | Loss: 0.12\n",
      "Epoch: 110/200 | Batch 000/002 | Loss: 0.08\n",
      "Epoch: 110/200 | Batch 001/002 | Loss: 0.14\n",
      "Epoch: 111/200 | Batch 000/002 | Loss: 0.11\n",
      "Epoch: 111/200 | Batch 001/002 | Loss: 0.10\n",
      "Epoch: 112/200 | Batch 000/002 | Loss: 0.08\n",
      "Epoch: 112/200 | Batch 001/002 | Loss: 0.14\n",
      "Epoch: 113/200 | Batch 000/002 | Loss: 0.12\n",
      "Epoch: 113/200 | Batch 001/002 | Loss: 0.10\n",
      "Epoch: 114/200 | Batch 000/002 | Loss: 0.14\n",
      "Epoch: 114/200 | Batch 001/002 | Loss: 0.07\n",
      "Epoch: 115/200 | Batch 000/002 | Loss: 0.11\n",
      "Epoch: 115/200 | Batch 001/002 | Loss: 0.11\n",
      "Epoch: 116/200 | Batch 000/002 | Loss: 0.10\n",
      "Epoch: 116/200 | Batch 001/002 | Loss: 0.11\n",
      "Epoch: 117/200 | Batch 000/002 | Loss: 0.12\n",
      "Epoch: 117/200 | Batch 001/002 | Loss: 0.09\n",
      "Epoch: 118/200 | Batch 000/002 | Loss: 0.12\n",
      "Epoch: 118/200 | Batch 001/002 | Loss: 0.08\n",
      "Epoch: 119/200 | Batch 000/002 | Loss: 0.13\n",
      "Epoch: 119/200 | Batch 001/002 | Loss: 0.08\n",
      "Epoch: 120/200 | Batch 000/002 | Loss: 0.06\n",
      "Epoch: 120/200 | Batch 001/002 | Loss: 0.14\n",
      "Epoch: 121/200 | Batch 000/002 | Loss: 0.08\n",
      "Epoch: 121/200 | Batch 001/002 | Loss: 0.12\n",
      "Epoch: 122/200 | Batch 000/002 | Loss: 0.09\n",
      "Epoch: 122/200 | Batch 001/002 | Loss: 0.12\n",
      "Epoch: 123/200 | Batch 000/002 | Loss: 0.13\n",
      "Epoch: 123/200 | Batch 001/002 | Loss: 0.07\n",
      "Epoch: 124/200 | Batch 000/002 | Loss: 0.09\n",
      "Epoch: 124/200 | Batch 001/002 | Loss: 0.11\n",
      "Epoch: 125/200 | Batch 000/002 | Loss: 0.12\n",
      "Epoch: 125/200 | Batch 001/002 | Loss: 0.08\n",
      "Epoch: 126/200 | Batch 000/002 | Loss: 0.08\n",
      "Epoch: 126/200 | Batch 001/002 | Loss: 0.12\n",
      "Epoch: 127/200 | Batch 000/002 | Loss: 0.08\n",
      "Epoch: 127/200 | Batch 001/002 | Loss: 0.12\n",
      "Epoch: 128/200 | Batch 000/002 | Loss: 0.13\n",
      "Epoch: 128/200 | Batch 001/002 | Loss: 0.07\n",
      "Epoch: 129/200 | Batch 000/002 | Loss: 0.12\n",
      "Epoch: 129/200 | Batch 001/002 | Loss: 0.08\n",
      "Epoch: 130/200 | Batch 000/002 | Loss: 0.09\n",
      "Epoch: 130/200 | Batch 001/002 | Loss: 0.10\n",
      "Epoch: 131/200 | Batch 000/002 | Loss: 0.08\n",
      "Epoch: 131/200 | Batch 001/002 | Loss: 0.11\n",
      "Epoch: 132/200 | Batch 000/002 | Loss: 0.09\n",
      "Epoch: 132/200 | Batch 001/002 | Loss: 0.10\n",
      "Epoch: 133/200 | Batch 000/002 | Loss: 0.12\n",
      "Epoch: 133/200 | Batch 001/002 | Loss: 0.07\n",
      "Epoch: 134/200 | Batch 000/002 | Loss: 0.09\n",
      "Epoch: 134/200 | Batch 001/002 | Loss: 0.10\n",
      "Epoch: 135/200 | Batch 000/002 | Loss: 0.11\n",
      "Epoch: 135/200 | Batch 001/002 | Loss: 0.08\n",
      "Epoch: 136/200 | Batch 000/002 | Loss: 0.08\n",
      "Epoch: 136/200 | Batch 001/002 | Loss: 0.11\n",
      "Epoch: 137/200 | Batch 000/002 | Loss: 0.09\n",
      "Epoch: 137/200 | Batch 001/002 | Loss: 0.10\n",
      "Epoch: 138/200 | Batch 000/002 | Loss: 0.10\n",
      "Epoch: 138/200 | Batch 001/002 | Loss: 0.08\n",
      "Epoch: 139/200 | Batch 000/002 | Loss: 0.10\n",
      "Epoch: 139/200 | Batch 001/002 | Loss: 0.09\n",
      "Epoch: 140/200 | Batch 000/002 | Loss: 0.08\n",
      "Epoch: 140/200 | Batch 001/002 | Loss: 0.10\n",
      "Epoch: 141/200 | Batch 000/002 | Loss: 0.11\n",
      "Epoch: 141/200 | Batch 001/002 | Loss: 0.08\n",
      "Epoch: 142/200 | Batch 000/002 | Loss: 0.10\n",
      "Epoch: 142/200 | Batch 001/002 | Loss: 0.08\n",
      "Epoch: 143/200 | Batch 000/002 | Loss: 0.06\n",
      "Epoch: 143/200 | Batch 001/002 | Loss: 0.12\n",
      "Epoch: 144/200 | Batch 000/002 | Loss: 0.08\n",
      "Epoch: 144/200 | Batch 001/002 | Loss: 0.10\n",
      "Epoch: 145/200 | Batch 000/002 | Loss: 0.09\n",
      "Epoch: 145/200 | Batch 001/002 | Loss: 0.09\n",
      "Epoch: 146/200 | Batch 000/002 | Loss: 0.12\n",
      "Epoch: 146/200 | Batch 001/002 | Loss: 0.06\n",
      "Epoch: 147/200 | Batch 000/002 | Loss: 0.10\n",
      "Epoch: 147/200 | Batch 001/002 | Loss: 0.08\n",
      "Epoch: 148/200 | Batch 000/002 | Loss: 0.09\n",
      "Epoch: 148/200 | Batch 001/002 | Loss: 0.08\n",
      "Epoch: 149/200 | Batch 000/002 | Loss: 0.13\n",
      "Epoch: 149/200 | Batch 001/002 | Loss: 0.05\n",
      "Epoch: 150/200 | Batch 000/002 | Loss: 0.07\n",
      "Epoch: 150/200 | Batch 001/002 | Loss: 0.11\n",
      "Epoch: 151/200 | Batch 000/002 | Loss: 0.08\n",
      "Epoch: 151/200 | Batch 001/002 | Loss: 0.09\n",
      "Epoch: 152/200 | Batch 000/002 | Loss: 0.08\n",
      "Epoch: 152/200 | Batch 001/002 | Loss: 0.09\n",
      "Epoch: 153/200 | Batch 000/002 | Loss: 0.10\n",
      "Epoch: 153/200 | Batch 001/002 | Loss: 0.07\n",
      "Epoch: 154/200 | Batch 000/002 | Loss: 0.09\n",
      "Epoch: 154/200 | Batch 001/002 | Loss: 0.08\n",
      "Epoch: 155/200 | Batch 000/002 | Loss: 0.08\n",
      "Epoch: 155/200 | Batch 001/002 | Loss: 0.09\n",
      "Epoch: 156/200 | Batch 000/002 | Loss: 0.11\n",
      "Epoch: 156/200 | Batch 001/002 | Loss: 0.06\n",
      "Epoch: 157/200 | Batch 000/002 | Loss: 0.07\n",
      "Epoch: 157/200 | Batch 001/002 | Loss: 0.10\n",
      "Epoch: 158/200 | Batch 000/002 | Loss: 0.08\n",
      "Epoch: 158/200 | Batch 001/002 | Loss: 0.09\n",
      "Epoch: 159/200 | Batch 000/002 | Loss: 0.08\n",
      "Epoch: 159/200 | Batch 001/002 | Loss: 0.09\n",
      "Epoch: 160/200 | Batch 000/002 | Loss: 0.07\n",
      "Epoch: 160/200 | Batch 001/002 | Loss: 0.10\n",
      "Epoch: 161/200 | Batch 000/002 | Loss: 0.05\n",
      "Epoch: 161/200 | Batch 001/002 | Loss: 0.11\n",
      "Epoch: 162/200 | Batch 000/002 | Loss: 0.09\n",
      "Epoch: 162/200 | Batch 001/002 | Loss: 0.08\n",
      "Epoch: 163/200 | Batch 000/002 | Loss: 0.09\n",
      "Epoch: 163/200 | Batch 001/002 | Loss: 0.07\n",
      "Epoch: 164/200 | Batch 000/002 | Loss: 0.07\n",
      "Epoch: 164/200 | Batch 001/002 | Loss: 0.10\n",
      "Epoch: 165/200 | Batch 000/002 | Loss: 0.06\n",
      "Epoch: 165/200 | Batch 001/002 | Loss: 0.11\n",
      "Epoch: 166/200 | Batch 000/002 | Loss: 0.07\n",
      "Epoch: 166/200 | Batch 001/002 | Loss: 0.09\n",
      "Epoch: 167/200 | Batch 000/002 | Loss: 0.10\n",
      "Epoch: 167/200 | Batch 001/002 | Loss: 0.07\n",
      "Epoch: 168/200 | Batch 000/002 | Loss: 0.09\n",
      "Epoch: 168/200 | Batch 001/002 | Loss: 0.07\n",
      "Epoch: 169/200 | Batch 000/002 | Loss: 0.09\n",
      "Epoch: 169/200 | Batch 001/002 | Loss: 0.07\n",
      "Epoch: 170/200 | Batch 000/002 | Loss: 0.07\n",
      "Epoch: 170/200 | Batch 001/002 | Loss: 0.09\n",
      "Epoch: 171/200 | Batch 000/002 | Loss: 0.08\n",
      "Epoch: 171/200 | Batch 001/002 | Loss: 0.07\n",
      "Epoch: 172/200 | Batch 000/002 | Loss: 0.07\n",
      "Epoch: 172/200 | Batch 001/002 | Loss: 0.09\n",
      "Epoch: 173/200 | Batch 000/002 | Loss: 0.11\n",
      "Epoch: 173/200 | Batch 001/002 | Loss: 0.05\n",
      "Epoch: 174/200 | Batch 000/002 | Loss: 0.08\n",
      "Epoch: 174/200 | Batch 001/002 | Loss: 0.07\n",
      "Epoch: 175/200 | Batch 000/002 | Loss: 0.10\n",
      "Epoch: 175/200 | Batch 001/002 | Loss: 0.06\n",
      "Epoch: 176/200 | Batch 000/002 | Loss: 0.09\n",
      "Epoch: 176/200 | Batch 001/002 | Loss: 0.07\n",
      "Epoch: 177/200 | Batch 000/002 | Loss: 0.08\n",
      "Epoch: 177/200 | Batch 001/002 | Loss: 0.08\n",
      "Epoch: 178/200 | Batch 000/002 | Loss: 0.05\n",
      "Epoch: 178/200 | Batch 001/002 | Loss: 0.10\n",
      "Epoch: 179/200 | Batch 000/002 | Loss: 0.07\n",
      "Epoch: 179/200 | Batch 001/002 | Loss: 0.08\n",
      "Epoch: 180/200 | Batch 000/002 | Loss: 0.06\n",
      "Epoch: 180/200 | Batch 001/002 | Loss: 0.09\n",
      "Epoch: 181/200 | Batch 000/002 | Loss: 0.07\n",
      "Epoch: 181/200 | Batch 001/002 | Loss: 0.09\n",
      "Epoch: 182/200 | Batch 000/002 | Loss: 0.10\n",
      "Epoch: 182/200 | Batch 001/002 | Loss: 0.05\n",
      "Epoch: 183/200 | Batch 000/002 | Loss: 0.08\n",
      "Epoch: 183/200 | Batch 001/002 | Loss: 0.07\n",
      "Epoch: 184/200 | Batch 000/002 | Loss: 0.09\n",
      "Epoch: 184/200 | Batch 001/002 | Loss: 0.06\n",
      "Epoch: 185/200 | Batch 000/002 | Loss: 0.07\n",
      "Epoch: 185/200 | Batch 001/002 | Loss: 0.08\n",
      "Epoch: 186/200 | Batch 000/002 | Loss: 0.09\n",
      "Epoch: 186/200 | Batch 001/002 | Loss: 0.06\n",
      "Epoch: 187/200 | Batch 000/002 | Loss: 0.08\n",
      "Epoch: 187/200 | Batch 001/002 | Loss: 0.06\n",
      "Epoch: 188/200 | Batch 000/002 | Loss: 0.08\n",
      "Epoch: 188/200 | Batch 001/002 | Loss: 0.07\n",
      "Epoch: 189/200 | Batch 000/002 | Loss: 0.04\n",
      "Epoch: 189/200 | Batch 001/002 | Loss: 0.11\n",
      "Epoch: 190/200 | Batch 000/002 | Loss: 0.08\n",
      "Epoch: 190/200 | Batch 001/002 | Loss: 0.07\n",
      "Epoch: 191/200 | Batch 000/002 | Loss: 0.05\n",
      "Epoch: 191/200 | Batch 001/002 | Loss: 0.10\n",
      "Epoch: 192/200 | Batch 000/002 | Loss: 0.06\n",
      "Epoch: 192/200 | Batch 001/002 | Loss: 0.09\n",
      "Epoch: 193/200 | Batch 000/002 | Loss: 0.06\n",
      "Epoch: 193/200 | Batch 001/002 | Loss: 0.08\n",
      "Epoch: 194/200 | Batch 000/002 | Loss: 0.10\n",
      "Epoch: 194/200 | Batch 001/002 | Loss: 0.05\n",
      "Epoch: 195/200 | Batch 000/002 | Loss: 0.06\n",
      "Epoch: 195/200 | Batch 001/002 | Loss: 0.09\n",
      "Epoch: 196/200 | Batch 000/002 | Loss: 0.10\n",
      "Epoch: 196/200 | Batch 001/002 | Loss: 0.04\n",
      "Epoch: 197/200 | Batch 000/002 | Loss: 0.09\n",
      "Epoch: 197/200 | Batch 001/002 | Loss: 0.06\n",
      "Epoch: 198/200 | Batch 000/002 | Loss: 0.07\n",
      "Epoch: 198/200 | Batch 001/002 | Loss: 0.07\n",
      "Epoch: 199/200 | Batch 000/002 | Loss: 0.07\n",
      "Epoch: 199/200 | Batch 001/002 | Loss: 0.07\n",
      "Epoch: 200/200 | Batch 000/002 | Loss: 0.10\n",
      "Epoch: 200/200 | Batch 001/002 | Loss: 0.04\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "torch.manual_seed(1)\n",
    "model = LogisticRegression(num_features=2)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.05)\n",
    "\n",
    "num_epochs = 200\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    model = model.train()\n",
    "    for batch_idx, (features, class_labels) in enumerate(train_loader):\n",
    "\n",
    "        probas = model(features)\n",
    "        \n",
    "        loss = F.binary_cross_entropy(probas, class_labels.view(probas.shape))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### LOGGING\n",
    "        print(f'Epoch: {epoch+1:03d}/{num_epochs:03d}'\n",
    "               f' | Batch {batch_idx:03d}/{len(train_loader):03d}'\n",
    "               f' | Loss: {loss:.2f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avaliando nossos Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0080],\n",
       "        [0.0163],\n",
       "        [0.0150],\n",
       "        [0.0092],\n",
       "        [0.0191],\n",
       "        [0.0716],\n",
       "        [0.9852],\n",
       "        [0.8961],\n",
       "        [0.8798],\n",
       "        [0.9704]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se probas > 0.5 --> classe=1 e se probas n√£o √© > 0.5 --> classe = 0\n",
    "pred = torch.where(probas > 0.5, 1, 0) # Threshold=0.5. 1 e 0 --> s√£o as nossas Classes \n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_labels.view(pred.shape).to(pred.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, dataloader):\n",
    "\n",
    "    model = model.eval()\n",
    "    \n",
    "    correct = 0.0\n",
    "    total_examples = 0\n",
    "    \n",
    "    for idx, (features, class_labels) in enumerate(dataloader):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            probas = model(features)\n",
    "        \n",
    "        pred = torch.where(probas > 0.5, 1, 0)\n",
    "        lab = class_labels.view(pred.shape).to(pred.dtype)\n",
    "\n",
    "        compare = lab == pred # Sabemos que em Python: int(True) = 1 e int(False) = 0\n",
    "        correct += torch.sum(compare)\n",
    "        total_examples += len(compare)\n",
    "\n",
    "    return correct / total_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = compute_accuracy(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.0%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Accuracy: {train_acc*100}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LightningAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
